challenges:
  - id: "001"
    description: "Setup a single node cluster using docker."
    setup: |
      Welcome, stranger. My name is Scylla, the NoSQL monster. <nom nom>
      I'm here to guide you through a series of challenges to make learning ScyllaDB fun!
      
      Here's some hints to get you started:
      
        docker network create --driver bridge scylla
        docker run --rm -d --name node1 --network scylla scylladb/scylla:6.1.1 --smp 1 --memory 1G
    solve:
      - "check_docker_network scylla"
      - "check_docker_container node1 scylla"
      - "check_nodetool_status node1"
  - id: "002"
    description: "Add 2 more nodes to the cluster."
    setup: |
      You can use the --seeds option to specify the IP address of the seed node. 
      This is the first node that you started.
      
        docker run --rm -d --name node2 --network scylla scylladb/scylla:6.1.1 --smp 1 --memory 1G --seeds=node1
        docker run --rm -d --name node3 --network scylla scylladb/scylla:6.1.1 --smp 1 --memory 1G --seeds=node1
    solve:
      - "check_docker_network scylla"
      - "check_docker_container node1 scylla"
      - "check_docker_container node2 scylla"
      - "check_docker_container node3 scylla"
      - "check_nodetool_status node1"
  - id: "003"
    description: "Creating a keyspace and table."
    setup: |
      Now that you have a 3-node cluster, let's create a keyspace and table.
      First, let's connect to node1 via cqlsh:

        docker exec -it node1 cqlsh

      Now, create a keyspace named 'catalog' with a replication factor of 3.

        CREATE KEYSPACE IF NOT EXISTS catalog
        WITH REPLICATION = {
          'class': 'NetworkTopologyStrategy', 
          'replication_factor': 3
        };
      
      Once you have created the keyspace, create a table named 'mutant_data' with the following schema:
      
        CREATE TABLE catalog.mutant_data (
          first_name text,
          last_name text,
          address text,
          picture_location text,
          PRIMARY KEY((first_name, last_name))
        );
    solve:
      - "check_keyspace node1 catalog"
      - "check_table node1 catalog mutant_data"
  - id: "004"
    description: "Inserting data into the table."
    setup: |
      Now that you have a table, let's insert some data into it.
      Insert the following data into the 'mutant_data' table:
      
        INSERT INTO catalog.mutant_data ("first_name", "last_name", "address", "picture_location")
          VALUES ('Bob', 'Loblaw', '1313 Mockingbird Lane', 'www.facebook.com/bobloblaw');
        INSERT INTO catalog.mutant_data ("first_name", "last_name", "address", "picture_location")
          VALUES ('Bob', 'Zemuda', '1202 Coffman Lane', 'www.facebook.com/bzemuda');
        INSERT INTO catalog.mutant_data ("first_name", "last_name", "address", "picture_location")
          VALUES ('Jim', 'Jeffries', '1211 Hollywood Lane', 'www.facebook.com/jeffries');
    solve:
      - "check_data node1 catalog mutant_data 3"
  - id: "005"
    description: "Querying and updating data from the table."
    setup: |
      Now that you have data in the table, let's query it.
      Query the 'mutant_data' table to get the address of Bob Loblaw.
      
        SELECT address FROM catalog.mutant_data WHERE first_name = 'Bob' AND last_name = 'Loblaw';
      
      You should get the address '1313 Mockingbird Lane'.
      
      Now, update the address of Bob Loblaw to '1234 Elm Street'.
      
        UPDATE catalog.mutant_data SET address = '1234 Elm Street' WHERE first_name = 'Bob' AND last_name = 'Loblaw';
  - id: "006"
    description: "Hot partitions."
    setup: |
      Hot partitions are partitions that receive a disproportionate amount of reads or writes.
      This can lead to performance issues. Let's create a table that could potentially have hot partitions.
      
        CREATE TABLE events (
          user_id UUID,
          event_id UUID,
          event_timestamp TIMESTAMP,
          event_type TEXT,
          event_data TEXT,
          PRIMARY KEY (user_id, event_timestamp)
        );
      
      Insert some data into the 'events' table:
        USER_ID="123e4567-e89b-12d3-a456-426614174000"  
        while true; do
          cqlsh -e "INSERT INTO catalog.events (user_id, event_id, event_timestamp, event_type, event_data)
          VALUES ($USER_ID, uuid(), toTimestamp(now()), 'event_type_example', '$(head -c 10000 < /dev/urandom | base64)');";
        done
      
      This will insert random data into the 'events' table. You can stop the loop after a few seconds.
      
      Take a look at the top partitions:
         docker exec -it node1 nodetool toppartitions
      
      You should see the 'events' table listed as a hot partition.
        WRITES Sampler:
          Cardinality: ~3 (256 capacity)
          Top 10 partitions:
                Partition                                                 Count       +/-
                (catalog:events) 123e4567-e89b-12d3-a456-426614174000        14         0
                (system:peers) 172.18.0.3                                     5         0
                (system:peers) 172.18.0.4                                     4         0


      To fix this, you can add a time bucket to the primary key:
      
        CREATE TABLE events (
          user_id UUID,
          bucket_minute TIMESTAMP,
          event_id UUID,
          event_timestamp TIMESTAMP,
          event_type TEXT,
          event_data TEXT,
          PRIMARY KEY ((user_id, bucket_minute), event_timestamp)
        );
      
      Insert some data into the 'events' table:
        USER_ID="123e4567-e89b-12d3-a456-426614174000"  
        while true; do
          BUCKET_MINUTE=$(date +"%Y-%m-%d %H:%M:00")  # Current timestamp rounded to the minute
          cqlsh -e "INSERT INTO catalog.events (user_id, bucket_minute, event_id, event_timestamp, event_type, event_data)
            VALUES ($USER_ID, '$BUCKET_MINUTE', uuid(), toTimestamp(now()), 'event_type_example', '$(head -c 10000 < /dev/urandom | base64)');";
        done

      With this setup, the data is bucketed by each minute, allowing better distribution while keeping high-frequency events 
      grouped by the minute. This approach is useful in scenarios where you expect many events per user in a short time span, 
      effectively reducing the chance of a single hot partition.